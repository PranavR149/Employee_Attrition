{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67427b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRandomForestModel\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81       251\n",
      "           1       0.30      0.70      0.42        43\n",
      "\n",
      "    accuracy                           0.72       294\n",
      "   macro avg       0.62      0.71      0.62       294\n",
      "weighted avg       0.84      0.72      0.76       294\n",
      "\n",
      "\u001b[1mXGBModel\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80       251\n",
      "           1       0.30      0.77      0.43        43\n",
      "\n",
      "    accuracy                           0.70       294\n",
      "   macro avg       0.62      0.73      0.62       294\n",
      "weighted avg       0.85      0.70      0.75       294\n",
      "\n",
      "\u001b[1mAdaBoostModel\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.78      0.84       251\n",
      "           1       0.31      0.58      0.41        43\n",
      "\n",
      "    accuracy                           0.75       294\n",
      "   macro avg       0.61      0.68      0.62       294\n",
      "weighted avg       0.83      0.75      0.78       294\n",
      "\n",
      "\u001b[1mLogisticRegressionModel\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82       251\n",
      "           1       0.30      0.70      0.42        43\n",
      "\n",
      "    accuracy                           0.72       294\n",
      "   macro avg       0.62      0.71      0.62       294\n",
      "weighted avg       0.84      0.72      0.76       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class FeatureSelector:\n",
    "    \"\"\"\n",
    "    Feature Selector Class: Selects top features based on importance scores.\n",
    "\n",
    "    Inputs:\n",
    "    - n_estimators: int, Number of trees in the forest (default: 100).\n",
    "    - random_state: int, Seed for random number generator (default: 42).\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators: int = 100, random_state: int = 42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "    def select_features(self, X: pd.DataFrame, y: pd.Series, top_n: int = 28) -> list:\n",
    "        \"\"\"\n",
    "        Selects top features based on importance scores.\n",
    "\n",
    "        Inputs:\n",
    "        - X: pd.DataFrame, Features DataFrame.\n",
    "        - y: pd.Series, Target Series.\n",
    "        - top_n: int, Number of top features to select (default: 28).\n",
    "\n",
    "        Output:\n",
    "        - List of selected top features.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        # Train the model\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        # Get feature importances\n",
    "        feature_importances = self.model.feature_importances_\n",
    "\n",
    "        # Create a DataFrame of feature importances\n",
    "        feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "        \n",
    "        # Sort features by importance (descending order)\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        # Select top N features\n",
    "        top_n_features = feature_importance_df.head(top_n)['Feature'].tolist()\n",
    "\n",
    "        return top_n_features\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Model Evaluator Class: Evaluates model performance and sets threshold.\n",
    "\n",
    "    Inputs:\n",
    "    - model: Any, Machine learning model.\n",
    "    - X_train, X_val, X_test: pd.DataFrame, Features for training, validation, and testing.\n",
    "    - y_train, y_val, y_test: pd.Series, Target variables for training, validation, and testing.\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, model: any, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame,\n",
    "                 y_train: pd.Series, y_val: pd.Series, y_test: pd.Series):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.y_test = y_test\n",
    "        self.best_threshold = None\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains the model.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluates model performance on validation data.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        y_pred_val_proba = self.model.predict_proba(self.X_val)\n",
    "        if self.best_threshold is None:\n",
    "            threshold = 0.5  # Default threshold\n",
    "        else:\n",
    "            threshold = self.best_threshold\n",
    "        y_pred_best_threshold = (y_pred_val_proba[:, 1] >= threshold).astype(int)\n",
    "        return classification_report(self.y_val, y_pred_best_threshold)\n",
    "\n",
    "    def evaluate_test(self):\n",
    "        \"\"\"Evaluates model performance on test data.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        y_pred_test_proba = self.model.predict_proba(self.X_test)\n",
    "        if self.best_threshold is None:\n",
    "            threshold = 0.5  # Default threshold\n",
    "        else:\n",
    "            threshold = self.best_threshold\n",
    "        y_pred_best_threshold = (y_pred_test_proba[:, 1] >= threshold).astype(int)\n",
    "        return classification_report(self.y_test, y_pred_best_threshold)\n",
    "\n",
    "    def set_threshold(self, threshold: float):\n",
    "        \"\"\"Sets the threshold for model classification.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        self.best_threshold = threshold\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    \"\"\"Base Model Class: Defines common methods for all models.\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, model: any):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"Fits the model to the training data.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Predicts probabilities for the given data.\n",
    "\n",
    "        Access Level:\n",
    "        - Public\n",
    "        \"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "class RandomForestModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Random Forest Model Class: Wrapper for RandomForestClassifier.\n",
    "\n",
    "    Inputs:\n",
    "    - n_estimators: int, Number of trees in the forest (default: 200).\n",
    "    - random_state: int, Seed for random number generator (default: 42).\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators: int = 200, random_state: int = 42):\n",
    "        super().__init__(RandomForestClassifier(n_estimators=n_estimators, random_state=random_state))\n",
    "\n",
    "\n",
    "class XGBModel(BaseModel):\n",
    "    \"\"\"\n",
    "    XGBoost Model Class: Wrapper for XGBClassifier.\n",
    "\n",
    "    Inputs:\n",
    "    - n_estimators: int, Number of boosting rounds (default: 200).\n",
    "    - random_state: int, Seed for random number generator (default: 42).\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators: int = 200, random_state: int = 42):\n",
    "        super().__init__(XGBClassifier(n_estimators=n_estimators, random_state=random_state))\n",
    "\n",
    "\n",
    "class AdaBoostModel(BaseModel):\n",
    "    \"\"\"\n",
    "    AdaBoost Model Class: Wrapper for AdaBoostClassifier.\n",
    "\n",
    "    Inputs:\n",
    "    - n_estimators: int, Number of boosting rounds (default: 180).\n",
    "    - learning_rate: float, Learning rate shrinks the contribution of each classifier (default: 0.01).\n",
    "    - random_state: int, Seed for random number generator (default: 42).\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators: int = 180, learning_rate: float = 0.01, random_state: int = 42):\n",
    "        super().__init__(AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state))\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Logistic Regression Model Class: Wrapper for LogisticRegression.\n",
    "\n",
    "    Inputs:\n",
    "    - penalty: str, Regularization term (default: 'l1').\n",
    "    - C: float, Inverse of regularization strength (default: 0.1).\n",
    "    - solver: str, Algorithm to use in the optimization problem (default: 'liblinear').\n",
    "\n",
    "    Access Level:\n",
    "    - Public\n",
    "    \"\"\"\n",
    "    def __init__(self, penalty: str = 'l1', C: float = 0.1, solver: str = 'liblinear'):\n",
    "        super().__init__(LogisticRegression(penalty=penalty, C=C, solver=solver))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset = pd.read_csv(\"Employee.csv\")\n",
    "    \n",
    "    # Separate features (X) and target variable (y)\n",
    "    columns_to_keep = [\"MonthlyIncome\",\n",
    "        \"Age\",\n",
    "        \"DailyRate\",\n",
    "        \"DistanceFromHome\",\n",
    "        \"OverTime_Num\",\n",
    "        \"TotalWorkingYears\",\n",
    "        \"NumCompaniesWorked\",\n",
    "        \"YearsAtCompany\",\n",
    "        \"YearsWithCurrManager\",\n",
    "        \"StockOptionLevel\",\n",
    "        \"PercentSalaryHike\",\n",
    "        \"YearsInCurrentRole\",\n",
    "        \"EnvironmentSatisfaction\",\n",
    "        \"New EducationField\",\n",
    "        \"TrainingTimesLastYear\",\n",
    "        \"New JobRole\",\n",
    "        \"YearsSinceLastPromotion\",\n",
    "        \"RelationshipSatisfaction\",\n",
    "        \"JobInvolvement\",\n",
    "        \"JobSatisfaction\",\n",
    "        \"Education\",\n",
    "        \"WorkLifeBalance\",\n",
    "        \"Travel_Frequently\",\n",
    "        \"Married\",\n",
    "        \"Gender_Num\",\n",
    "        \"Sales Representative\",\n",
    "        \"Divorced\",\n",
    "        \"Laboratory Technician\",\n",
    "        \"Travel_Rarely\",\n",
    "        \"Sales Executive\",\n",
    "        \"Research Scientist\",\n",
    "        \"PerformanceRating\",\n",
    "        \"Non-Travel\",\n",
    "        \"Human Resources\",\n",
    "        \"Manufacturing Director\",\n",
    "        \"Healthcare Representative\",\n",
    "        \"Manager\",\n",
    "        \"Research Director\" ]\n",
    "    X = dataset[columns_to_keep]\n",
    "    y = dataset[\"Attrition_numeric\"]\n",
    "    \n",
    "    # Train-Validation-Test split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Create FeatureSelector instance\n",
    "    feature_selector = FeatureSelector()\n",
    "\n",
    "    # Select top features\n",
    "    selected_features = feature_selector.select_features(X_train, y_train)\n",
    "\n",
    "    models = [\n",
    "        RandomForestModel(),\n",
    "        XGBModel(),\n",
    "        AdaBoostModel(),\n",
    "        LogisticRegressionModel()\n",
    "    ]\n",
    "\n",
    "    thresholds = [0.19, 0.013, 0.328, 0.18]  # Thresholds for each model\n",
    "\n",
    "    for model, threshold in zip(models, thresholds):\n",
    "        evaluator = ModelEvaluator(model.model, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        evaluator.train()\n",
    "        evaluator.set_threshold(threshold)\n",
    "        print(\"\\033[1m\" + type(model).__name__ + \"\\033[0m\")  # Printing model name in bold\n",
    "        print(evaluator.evaluate_test())  # Evaluating model on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
